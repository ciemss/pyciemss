{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669c02c9",
   "metadata": {},
   "source": [
    "# Ensemble Challenge: Timepoint 2\n",
    "\n",
    "Goal: to capture the complexity and nuances around the evolution of the pandemic at various stages and locations.\n",
    "\n",
    "Location A: New York State\n",
    "Consider the following settings:\n",
    "Timepoint 1: April 3, 2020. Setting: New York State at the beginning of the pandemic when masking was the main preventative measure. No vaccines available.\n",
    "\n",
    "Timepoint 2: July 15, 2021. Setting: New York State upon the arrival of the Delta variant. Vaccines available.\n",
    "\n",
    "Timepoint 3: January 4, 2022. Setting: New York State coinciding with the arrival of the first Omicron wave. At-home testing widely available.\n",
    "\n",
    "BONUS: Consider the same three time points, but change the setting to Texas, which had different COVID-19 dynamics compared to the Northeastern states.\n",
    "\n",
    "...and related questions for each:\n",
    "What is the most relevant data to use for model calibration?\n",
    "What was our understanding of COVID-19 viral mechanisms at the time? For example, early in the pandemic, we didn't know if reinfection was a common occurance, or even possible.\n",
    "What are the parameters related to contagiousness/transmissibility and severity of the dominant strain at the time?\n",
    "What policies were in place for a stated location, and how can this information be incorporated into models? (See https://www.bsg.ox.ac.uk/research/covid-19-government-response-tracker for time series of interventions).\n",
    "Also--this is a good source for parameters: https://www.cdc.gov/coronavirus/2019-ncov/hcp/planning-scenarios.html\n",
    "\n",
    "For each timepoint:\n",
    "(a) Take a single model, calibrate it using any historical data prior to the given date, and create a 4-week forecast for cases, hospitalizations, and deaths beginning on the given date. (b) Evaluate the forecast using the COVID-19 Forecasting Hub Error Metrics (WIS, MAE). The single model evaluation should be done in the same way as the ensemble.\n",
    "\n",
    "Repeat (1), but with an ensemble of different models.\n",
    "\n",
    "a. It is fine to calibrate each model independently and weight naively.\n",
    "\n",
    "b. It would also be fine to calibrate the ensemble as a whole, assigning weights to the different component models, so that you minimize the error of the ensemble vs. historical data.\n",
    "\n",
    "c. Use the calibration scores and error metrics computed by the CDC Forecasting Hub. As stated on their website:\n",
    "\n",
    "“Periodically, we evaluate the accuracy and precision of the ensemble forecast and component models over recent and historical forecasting periods. Models forecasting incident hospitalizations at a national and state level are evaluated using adjusted relative weighted interval scores (WIS, a measure of distributional accuracy), and adjusted relative mean absolute error (MAE), and calibration scores. Scores are evaluated across weeks, locations, and targets. You can read a paper explaining these procedures in more detail, and look at the most recent monthly evaluation reports. The final report that includes case and death forecast evaluations is 2023-03-13.”\n",
    "\n",
    "Produce the forecast outputs in the format specified by the CDC forecasting challenge, including the specified quantiles.\n",
    "\n",
    "Data\n",
    "Use the following data sources:\n",
    "\n",
    "Cases: Johns Hopkins, Reich Lab (pulled from Johns Hopkins, but formatted)\n",
    "\n",
    "Hospitalizations: HealthData.gov\n",
    "\n",
    "Deaths: Johns Hopkins, Reich Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda4fda-2bd7-468c-8e91-d0d4323074c2",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13880386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyciemss.Ensemble.interfaces import (\n",
    "    load_and_sample_petri_ensemble, load_and_calibrate_and_sample_ensemble_model\n",
    ")\n",
    "from pyciemss.PetriNetODE.interfaces import (\n",
    "    load_and_sample_petri_model,\n",
    "    load_and_calibrate_and_sample_petri_model,\n",
    "    load_and_optimize_and_sample_petri_model,\n",
    "    load_and_calibrate_and_optimize_and_sample_petri_model\n",
    ")\n",
    "from pyciemss.visuals import plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab9bd4-3f9d-42e0-8fd5-e91bff3d81ba",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4942e1-ac95-4c72-9233-dd004cdf75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/DARPA-ASKEM/experiments/main/thin-thread-examples/milestone_12month/evaluation/ensemble_eval_SA/datasets/aabb3684-a7ea-4f60-98f1-a8e673ad6df5/dataset.csv'\n",
    "ny_data = pd.read_csv(url)\n",
    "\n",
    "# Grab test data for four-week forecast (07/15/2021 - 05/01/2020)\n",
    "test_data = ny_data[0:569].drop(columns=\"timestep\")\n",
    "\n",
    "# Select historical data up to Timepoint 2, 07/14/2021 (the first 541 rows)\n",
    "ny_data = ny_data[0:540]\n",
    "ny_data[[\"I\", \"H\", \"D\"]].to_csv(\"NY_data2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d809e73-ddcc-409e-8378-5a4047c1b50e",
   "metadata": {},
   "source": [
    "## Set up timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c142f-dfea-498f-a9b2-342fbee162ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timepoint = 0\n",
    "stop_timepoint = 540 + 28 # simulate for four weeks after end of data\n",
    "timepoints = [float(i) for i in range(stop_timepoint + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1a51e-ccb6-4d74-8e8c-71893a1975a7",
   "metadata": {},
   "source": [
    "## Select relevant models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459be6a2-d0ba-457f-9d47-dbd63a2ab57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_location = \"../../notebook/ensemble_eval_sa/SEIRHDS_V1.json\"\n",
    "model2_location = \"../../notebook/ensemble_eval_sa/SEIRHDS_basic_config.json\"\n",
    "model3_location = \"../../notebook/ensemble_eval_sa/SIRHD_age_stratified.json\"\n",
    "model4_location = \"../../notebook/ensemble_eval_sa/SIRHD_vacc_var_V1.json\"\n",
    "model5_location = \"../../notebook/ensemble_eval_sa/SIRHD_age_vacc_var.json\"\n",
    "model6_location = \"../../notebook/ensemble_eval_sa/SIRHD_mask_V3.json\"\n",
    "model7_location = \"../../notebook/ensemble_eval_sa/SIRHD_V1C.json\"\n",
    "model8_location = \"../../notebook/ensemble_eval_sa/SEIRD_ymo_age_strat.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a860cfb-04e4-48f8-8912-c2c5565e1faa",
   "metadata": {},
   "source": [
    "## Load, calibrate and sample an ensemble of one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328b097-70b7-41fc-98fc-1d6a7a06824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2\n",
    "model_paths = [model1_location]\n",
    "data_path = \"../../notebook/ensemble_eval_sa/NY_data2.csv\"\n",
    "weights = [1]\n",
    "solution_mappings = [{\"I\": \"I\", \"H\": \"H\", \"D\": \"D\"}] # \"column name in data\": \"observable or state variable in model\"\n",
    "\n",
    "# Run the calibration and sampling\n",
    "result = load_and_calibrate_and_sample_ensemble_model(\n",
    "    model_paths,\n",
    "    data_path,\n",
    "    weights,\n",
    "    solution_mappings,\n",
    "    num_samples,\n",
    "    timepoints,\n",
    "    verbose=True,\n",
    "    total_population=19340000,\n",
    "    num_iterations=100,\n",
    "    time_unit=\"days\",\n",
    "    visual_options={\"title\": \"Calibrated Ensemble\", \"subset\":\".*_sol\"}\n",
    ")\n",
    "\n",
    "# # Save results\n",
    "# result[\"data\"].to_csv(\n",
    "#     os.path.join(DEMO_PATH, \"results_petri_ensemble/calibrated_sample_results.csv\"), index=False\n",
    "# )\n",
    "# result[\"quantiles\"].to_csv(\n",
    "#     os.path.join(DEMO_PATH, \"results_petri_ensemble/calibrated_quantile_results.csv\"), index=False\n",
    "# )\n",
    "\n",
    "# Plot results\n",
    "schema = plots.trajectories(pd.DataFrame(result[\"data\"]), subset=\".*_sol\",\n",
    "                            points=test_data.reset_index(drop=True).rename(columns={\"I\":\"I_data\", \"H\":\"H_data\", \"D\":\"D_data\"}),\n",
    "                           )\n",
    "schema = plots.pad(schema, 5)\n",
    "plots.ipy_display(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f9a21-870a-4b07-804b-1c44d474c094",
   "metadata": {},
   "source": [
    "## Load, calibrate and sample an ensemble of several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10b282-a55a-4e72-afea-cb6c752f25c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss = 57055.510676562786\n",
      "iteration 25: loss = 47785.184825241566\n",
      "iteration 50: loss = 45456.67634254694\n",
      "iteration 75: loss = 44998.92005854845\n",
      "iteration 100: loss = 44675.045807898045\n"
     ]
    }
   ],
   "source": [
    "num_samples = 2\n",
    "model_paths = [model1_location, model4_location, model6_location]\n",
    "data_path = \"../../notebook/ensemble_eval_sa/NY_data2.csv\"\n",
    "weights = [1/3, 1/3, 1/3]\n",
    "solution_mappings = [{\"I\": \"I\", \"H\": \"H\", \"D\": \"D\"},\n",
    "                     {\"I\": \"I\", \"H\": \"H\", \"D\": \"D\"}, \n",
    "                     {\"I\": \"Cases\", \"H\": \"Hosp\", \"D\": \"Deaths\"},\n",
    "                     ]\n",
    "\n",
    "# Run the calibration and sampling\n",
    "result = load_and_calibrate_and_sample_ensemble_model(\n",
    "    model_paths,\n",
    "    data_path,\n",
    "    weights,\n",
    "    solution_mappings,\n",
    "    num_samples,\n",
    "    timepoints,\n",
    "    verbose=True,\n",
    "    total_population=19340000,\n",
    "    num_iterations=200,\n",
    "    time_unit=\"days\",\n",
    "    visual_options={\"title\": \"Calibrated Ensemble\", \"subset\":\".*_sol\"}\n",
    ")\n",
    "\n",
    "# # Save results\n",
    "# result[\"data\"].to_csv(\n",
    "#     os.path.join(DEMO_PATH, \"results_petri_ensemble/calibrated_sample_results.csv\"), index=False\n",
    "# )\n",
    "# result[\"quantiles\"].to_csv(\n",
    "#     os.path.join(DEMO_PATH, \"results_petri_ensemble/calibrated_quantile_results.csv\"), index=False\n",
    "# )\n",
    "\n",
    "# Plot results\n",
    "schema = plots.trajectories(pd.DataFrame(result[\"data\"]), subset=\".*_sol\",\n",
    "                            points=test_data.reset_index(drop=True).rename(columns={\"I\":\"I_data\", \"H\":\"H_data\", \"D\":\"D_data\"}),\n",
    "                           )\n",
    "schema = plots.pad(schema, 5)\n",
    "plots.ipy_display(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80201a8-7542-4532-a86f-c885c2e18073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
